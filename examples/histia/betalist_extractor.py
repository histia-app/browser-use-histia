from __future__ import annotations

"""
Agent dedicated to scraping newly published startups from BetaList.

This module mirrors the structure of ``appsumo_hot_extractor`` but adapts the
parsing primitives to BetaList's infinite-scroll listing. It performs a
deterministic scroll, extracts card HTML via ``evaluate`` and returns a strict
Pydantic report that can be consumed downstream or persisted as JSON.
"""

import argparse
import asyncio
import json
import os
import re
from datetime import UTC, date, datetime, time, timedelta
from pathlib import Path
from textwrap import dedent
from typing import Any, Iterable
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup
from bs4.element import Tag
from dotenv import load_dotenv
from pydantic import AnyHttpUrl, BaseModel, Field, ValidationError, field_serializer

from browser_use import Agent, Browser, ChatBrowserUse, ChatOpenAI
from browser_use.browser.events import NavigateToUrlEvent

try:
	import httpx  # type: ignore[import-not-found]
except ModuleNotFoundError:  # pragma: no cover - optional dependency for tests
	httpx = None

load_dotenv()

os.environ.setdefault('TIMEOUT_ScrollEvent', '20')
os.environ.setdefault('TIMEOUT_ScreenshotEvent', '45')
os.environ.setdefault('TIMEOUT_BrowserStateRequestEvent', '45')


class BetalistInput(BaseModel):
	"""Parameters provided by the user when launching the extractor."""

	url: AnyHttpUrl = Field(
		default=AnyHttpUrl('https://betalist.com/'),
		description='BetaList landing page that contains the infinite-scroll grid.',
	)
	last_days: int = Field(
		default=3,
		ge=1,
		le=30,
		description='Number of most recent days to keep (including today).',
	)
	max_startups: int = Field(
		default=200,
		ge=1,
		le=2000,
		description='Hard limit on the number of startups returned.',
	)
	output_path: Path = Field(
		default=Path('betalist_recent.json'),
		description='Destination file for the JSON snapshot.',
	)


class BetalistFounder(BaseModel):
	"""Minimal metadata about a founder/submitter listed on a card."""

	handle: str | None = Field(None, description='Displayed founder name or handle.')
	profile_url: str | None = Field(None, description='Absolute URL to the BetaList profile page.')


class BetalistStartup(BaseModel):
	"""Structured representation of a startup card."""

	startup_id: int = Field(..., description='Numeric identifier extracted from the DOM ID.')
	slug: str = Field(..., description='Slug portion of the /startups/<slug> URL.')
	name: str = Field(..., description='Startup name as rendered on the card.')
	startup_url: str = Field(..., description='Absolute URL to the startup detail page.')
	tagline: str | None = Field(None, description='Tagline/description text under the name.')
	image_url: str | None = Field(None, description='Primary thumbnail URL.')
	image_srcset: str | None = Field(None, description='Raw srcset attribute for retina assets.')
	image_alt: str | None = Field(None, description='Alt text associated with the thumbnail.')
	founders: list[BetalistFounder] = Field(default_factory=list, description='Founders/submitters associated with the listing.')
	published_label: str | None = Field(None, description='Human readable label such as ‚ÄúToday‚Äù or ‚ÄúNovember 20th‚Äù.')
	published_at: datetime | None = Field(None, description='UTC midnight for the listing date.')
	published_epoch: int | None = Field(None, description='Epoch seconds embedded in data-duplicate-id.')

	@field_serializer('published_at')
	def _serialize_published_at(self, value: datetime | None, _info) -> str | None:
		return value.isoformat().replace('+00:00', 'Z') if value else None


class BetalistReport(BaseModel):
	"""Complete response generated by the extractor."""

	source_url: AnyHttpUrl = Field(..., description='Page that was analysed.')
	generated_at: datetime = Field(default_factory=lambda: datetime.now(UTC), description='UTC timestamp when the run finished.')
	cutoff_date: date = Field(..., description='Earliest allowed date (inclusive).')
	startups: list[BetalistStartup] = Field(..., min_length=1, description='Ordered list of startups.')

	@field_serializer('source_url')
	def _serialize_source_url(self, value: AnyHttpUrl, _info) -> str:
		return str(value)

	@field_serializer('generated_at')
	def _serialize_generated_at(self, value: datetime, _info) -> str:
		return value.isoformat().replace('+00:00', 'Z')


def _clean_text(value: str | None) -> str | None:
	"""Collapse whitespace and strip empty strings."""
	if not value:
		return None
	text = re.sub(r'\s+', ' ', value).strip()
	return text or None


def _safe_attr(element: Tag | None, attribute: str) -> str | None:
	"""Return a string attribute value even when BeautifulSoup yields list types."""
	if not element:
		return None
	value = element.get(attribute)
	if isinstance(value, list):
		text = ' '.join(item for item in value if isinstance(item, str)).strip()
		return text or None
	if isinstance(value, str):
		return value
	return None


def _normalize_url(url: str | None, base_url: str) -> str | None:
	"""Return an absolute URL rooted on BetaList when needed."""
	if not url:
		return None
	url = url.strip()
	if not url:
		return None
	if url.startswith(('http://', 'https://')):
		return url
	if url.startswith('/'):
		parsed = urlparse(base_url)
		return f'{parsed.scheme}://{parsed.netloc}{url}'
	try:
		return urljoin(base_url, url)
	except Exception:
		return None


def _extract_numeric_id(dom_id: str | None) -> int | None:
	if not dom_id:
		return None
	match = re.search(r'(\d+)$', dom_id)
	if not match:
		return None
	try:
		return int(match.group(1))
	except ValueError:
		return None


def _duplicate_id_to_date(duplicate_id: str | None) -> tuple[int | None, date | None]:
	if not duplicate_id:
		return None, None
	match = re.search(r'(\d+)$', duplicate_id)
	if not match:
		return None, None
	try:
		epoch = int(match.group(1))
	except ValueError:
		return None, None
	try:
		return epoch, datetime.fromtimestamp(epoch, UTC).date()
	except (OverflowError, OSError):
		return epoch, None


MONTH_REGEX = re.compile(r'(january|february|march|april|may|june|july|august|september|october|november|december)\s+(\d{1,2})(?:st|nd|rd|th)?', re.IGNORECASE)


def _day_label_to_date(label: str | None, *, reference_year: int | None = None) -> date | None:
	if not label:
		return None
	match = MONTH_REGEX.search(label)
	if not match:
		return None
	month_name, day_str = match.groups()
	month_index = datetime.strptime(month_name.title(), '%B').month
	day = int(day_str)
	year = reference_year or datetime.now(UTC).year
	try:
		return date(year, month_index, day)
	except ValueError:
		return None


def _compute_cutoff(last_days: int) -> date:
	today = datetime.now(UTC).date()
	return today - timedelta(days=max(last_days - 1, 0))


STARTUP_EXTRACTION_SCRIPT = """() => {
  const grid = document.querySelector('.startupGrid');
  if (!grid) {
    return JSON.stringify({error: 'No .startupGrid found', count: 0, entries: []});
  }

  const entries = [];
  let currentDuplicateId = null;
  let currentLabel = null;

  const children = Array.from(grid.children);
  for (const node of children) {
    // Check for day header
    const duplicateId = node.getAttribute('data-duplicate-id');
    if (duplicateId) {
      currentDuplicateId = duplicateId;
      const strong = node.querySelector('strong');
      const text = node.textContent || '';
      currentLabel = text.trim();
      continue;
    }

    // Check for startup card
    if (!node.id || !node.id.startsWith('startup-')) {
      continue;
    }

    // Extract founders (from links like /@username)
    const founders = Array.from(node.querySelectorAll('a[href^="/@"]')).map((anchor) => {
      const srOnly = anchor.querySelector('.sr-only');
      const label = srOnly ? (srOnly.textContent || '').trim() : (anchor.textContent || '').trim();
      return {
        label: label,
        href: anchor.getAttribute('href'),
      };
    });

    entries.push({
      id: node.id,
      html: node.outerHTML,
      dayDuplicateId: currentDuplicateId,
      dayLabel: currentLabel,
      founders,
    });
  }

  return JSON.stringify({error: null, count: entries.length, entries: entries});
}"""


DAY_METADATA_SCRIPT = """() => {
  const labels = Array.from(document.querySelectorAll('.startupGrid [data-duplicate-id]')).map((node) => ({
    duplicateId: node.getAttribute('data-duplicate-id'),
    label: (node.textContent || '').trim(),
  }));
  return JSON.stringify(labels);
}"""


def _first_tag_with_class(root: Tag, class_fragment: str, tag_names: Iterable[str]) -> Tag | None:
	for tag in root.find_all(tag_names):
		class_attr = tag.get('class') or []
		if isinstance(class_attr, list):
			if any(class_fragment in str(cls) for cls in class_attr):
				return tag
		elif isinstance(class_attr, str) and class_fragment in class_attr:
			return tag
	return None


def _parse_startup_payload(payload: dict[str, Any], source_url: str) -> BetalistStartup | None:
	card_html = payload.get('html')
	if not isinstance(card_html, str):
		return None
	soup = BeautifulSoup(card_html, 'html.parser')
	card_root = soup.find(id=re.compile(r'^startup-')) or soup.find('div')

	if not card_root:
		return None

	startup_id = _extract_numeric_id(payload.get('id') or _safe_attr(card_root, 'id'))

	# Find name anchor - prefer the one with font-medium class (the main title link)
	name_anchor = card_root.select_one('a[href^="/startups/"][class*="font-medium"]') or card_root.select_one('a[href^="/startups/"]')
	name = _clean_text(name_anchor.get_text() if name_anchor else None)
	startup_url = _normalize_url(_safe_attr(name_anchor, 'href') if name_anchor else None, source_url)

	if not (startup_id and name and startup_url):
		return None

	parsed_url = urlparse(startup_url)
	slug = parsed_url.path.rstrip('/').split('/')[-1] if parsed_url.path else str(startup_id)

	# Find tagline - it's in an <a> with text-gray-500 class that links to the startup
	tagline_anchor = card_root.select_one('a[href^="/startups/"][class*="text-gray-500"]')
	if tagline_anchor and tagline_anchor != name_anchor:
		tagline = _clean_text(tagline_anchor.get_text())
	else:
		# Fallback to any element with text-gray-500
		tagline_elem = _first_tag_with_class(card_root, 'text-gray-500', ('a', 'div', 'p', 'span'))
		tagline = _clean_text(tagline_elem.get_text() if tagline_elem else None)

	# Find image - prefer img in the main card link area
	image_elem = card_root.select_one('a[href^="/startups/"] img') or card_root.find('img')
	image_url = _normalize_url(_safe_attr(image_elem, 'src') if image_elem else None, source_url)
	image_srcset = _safe_attr(image_elem, 'srcset') if image_elem else None
	image_alt = _clean_text(_safe_attr(image_elem, 'alt') if image_elem else None)

	founder_payload = payload.get('founders') or []
	founders: list[BetalistFounder] = []
	for founder in founder_payload:
		if not isinstance(founder, dict):
			continue
		handle = _clean_text(founder.get('label'))
		url = _normalize_url(founder.get('href'), source_url)
		if handle or url:
			founders.append(BetalistFounder(handle=handle, profile_url=url))

	epoch, epoch_date = _duplicate_id_to_date(payload.get('dayDuplicateId'))
	if not epoch_date:
		epoch_date = _day_label_to_date(payload.get('dayLabel'))

	published_at = (
		datetime.combine(epoch_date, time(0, 0, tzinfo=UTC))
		if epoch_date
		else None
	)

	return BetalistStartup(
		startup_id=startup_id,
		slug=slug,
		name=name,
		startup_url=startup_url,
		tagline=tagline,
		image_url=image_url,
		image_srcset=image_srcset,
		image_alt=image_alt,
		founders=founders,
		published_label=_clean_text(payload.get('dayLabel')),
		published_at=published_at,
		published_epoch=epoch,
	)


def _build_report_from_payloads(
	payloads: Iterable[dict[str, Any]],
	source_url: str,
	cutoff_date: date,
	max_startups: int,
) -> BetalistReport | None:
	startups: list[BetalistStartup] = []
	seen_slugs: set[str] = set()
	skipped_old = 0
	skipped_parse = 0

	for payload in payloads:
		if not isinstance(payload, dict):
			continue
		startup = _parse_startup_payload(payload, source_url)
		if not startup:
			skipped_parse += 1
			if skipped_parse == 1:
				# Log first failure for debugging
				payload_id = payload.get('id', 'unknown')
				print(f'‚ö†Ô∏è  Failed to parse first payload (id: {payload_id})')
			continue
		# Filter by date if available, otherwise include (assume recent if no date)
		if startup.published_at:
			if startup.published_at.date() < cutoff_date:
				skipped_old += 1
				continue
		slug_key = startup.slug.lower()
		if slug_key in seen_slugs:
			continue
		seen_slugs.add(slug_key)
		startups.append(startup)
		if len(startups) >= max_startups:
			break

	if skipped_parse > 0:
		print(f'‚ö†Ô∏è  Skipped {skipped_parse} entries due to parsing errors')
	if skipped_old > 0:
		print(f'‚ö†Ô∏è  Skipped {skipped_old} entries older than cutoff date')
	if not startups:
		print(f'‚ùå No startups passed filtering (cutoff: {cutoff_date.isoformat()})')
		return None

	return BetalistReport(
		source_url=AnyHttpUrl(source_url),
		cutoff_date=cutoff_date,
		startups=startups,
	)


async def _evaluate_json_script(page, script: str) -> list[Any]:
	result = await page.evaluate(script)
	if not result:
		return []
	if isinstance(result, str):
		try:
			parsed = json.loads(result)
			if isinstance(parsed, dict) and 'entries' in parsed:
				return parsed.get('entries', [])
			if isinstance(parsed, list):
				return parsed
		except json.JSONDecodeError:
			return []
	if isinstance(result, list):
		return result
	if isinstance(result, dict) and 'entries' in result:
		return result.get('entries', [])
	return []


async def _oldest_loaded_date(page) -> date | None:
	labels = await _evaluate_json_script(page, DAY_METADATA_SCRIPT)
	oldest: date | None = None
	for entry in labels:
		if not isinstance(entry, dict):
			continue
		_, epoch_date = _duplicate_id_to_date(entry.get('duplicateId'))
		if not epoch_date:
			epoch_date = _day_label_to_date(entry.get('label'))
		if not epoch_date:
			continue
		if oldest is None or epoch_date < oldest:
			oldest = epoch_date
	return oldest


async def _scroll_until_cutoff(browser: Browser, cutoff_date: date, *, max_scrolls: int = 80) -> None:
	page = await browser.get_current_page()
	if not page:
		raise RuntimeError('No active page available for scrolling.')

	viewport_height = await page.evaluate('() => window.innerHeight || document.documentElement.clientHeight') or 900
	viewport_height = int(viewport_height)

	last_position = -1
	stalled_scrolls = 0

	for scroll_count in range(max_scrolls):
		oldest = await _oldest_loaded_date(page)
		if oldest:
			if oldest <= cutoff_date:
				print(f'‚úÖ Reached cutoff date: oldest={oldest.isoformat()}, cutoff={cutoff_date.isoformat()}')
				break
			if scroll_count % 10 == 0:
				print(f'üìú Scroll {scroll_count}: oldest date={oldest.isoformat()}, target={cutoff_date.isoformat()}')

		current_position = await page.evaluate('() => window.pageYOffset || document.documentElement.scrollTop') or 0
		current_position = int(current_position)

		if current_position == last_position:
			stalled_scrolls += 1
			if stalled_scrolls >= 3:
				print(f'‚ö†Ô∏è  Scroll stalled after {scroll_count} attempts')
				break
		else:
			stalled_scrolls = 0

		last_position = current_position
		await page.evaluate(f'() => window.scrollBy(0, {viewport_height})')
		await asyncio.sleep(1.5)

	await asyncio.sleep(2)


async def _extract_via_dom(browser: Browser, task_input: BetalistInput, cutoff_date: date) -> BetalistReport | None:
	page = await browser.get_current_page()
	if not page:
		print('‚ö†Ô∏è  No active page for DOM extraction')
		return None

	payloads = await _evaluate_json_script(page, STARTUP_EXTRACTION_SCRIPT)
	if not payloads:
		print(f'‚ö†Ô∏è  No startup payloads extracted from DOM (found {len(payloads)} entries)')
		return None

	print(f'‚úÖ Extracted {len(payloads)} startup entries from DOM')
	report = _build_report_from_payloads(
		payloads,
		str(task_input.url),
		cutoff_date,
		task_input.max_startups,
	)
	if report:
		print(f'‚úÖ Built report with {len(report.startups)} startups after filtering')
	return report


async def run_betalist_extraction(task_input: BetalistInput) -> BetalistReport | None:
	if os.getenv('BROWSER_USE_API_KEY'):
		llm = ChatBrowserUse()
	else:
		model_name = os.getenv('OPENAI_MODEL', 'gemini-2.5-flash-lite-preview-09-2025')
		if 'gemini' not in model_name.lower():
			model_name = 'gemini-2.5-flash-lite-preview-09-2025'
		timeout_kwargs: dict[str, Any] = {}
		if httpx is not None:
			timeout_kwargs['timeout'] = httpx.Timeout(180.0, connect=60.0, read=180.0, write=30.0)
		llm = ChatOpenAI(
			model=model_name,
			**timeout_kwargs,
			max_retries=3,
			max_completion_tokens=90960,
			add_schema_to_system_prompt='gemini' in model_name.lower(),
			dont_force_structured_output='gemini' in model_name.lower(),
		)

	browser = Browser(headless=False)
	await browser.start()

	cutoff_date = _compute_cutoff(task_input.last_days)
	source_url = str(task_input.url)

	try:
		navigate_event = NavigateToUrlEvent(url=source_url, new_tab=False)
		await browser.event_bus.dispatch(navigate_event)
		await navigate_event

		page = await browser.get_current_page()
		if not page:
			page = await browser.new_page(source_url)

		await asyncio.sleep(5)
		print(f'üìú Scrolling until cutoff date: {cutoff_date.isoformat()}')
		await _scroll_until_cutoff(browser, cutoff_date)
		await asyncio.sleep(3)

		dom_report = await _extract_via_dom(browser, task_input, cutoff_date)
		if dom_report:
			return dom_report

		print('‚ö†Ô∏è  DOM extraction failed, falling back to Agent-based extraction...')
		task = dedent(
			f"""
			Tu es d√©j√† sur {source_url}. Ton objectif est d'extraire les startups apparues sur BetaList
			durant les {task_input.last_days} derniers jours (cutoff UTC: {cutoff_date.isoformat()}).

			Contraintes:
			- Ne navigue pas vers d'autres pages.
			- N'utilise pas `scroll` (le script a d√©j√† charg√© les donn√©es).
			- Utilise `evaluate` pour retourner un tableau JSON avec `outerHTML` de chaque carte startup.
			- Filtre toute carte dont la date (Today, Yesterday...) est ant√©rieure √† {cutoff_date.isoformat()}.
			- Limite la r√©ponse √† {task_input.max_startups} startups maximum.
			- Termine avec `done` et fournis un objet `BetalistReport` complet dans `data`.
			"""
		).strip()

		agent = Agent(
			task=task,
			llm=llm,
			browser=browser,
			output_model_schema=BetalistReport,
			use_vision='auto',
			vision_detail_level='auto',
			step_timeout=120,
			llm_timeout=90,
			max_failures=3,
			max_history_items=10,
			max_steps=10,
			directly_open_url=False,
		)
		history = await agent.run()

		if history.structured_output:
			return history.structured_output  # type: ignore[return-value]

		final_result = history.final_result()
		if final_result:
			try:
				return BetalistReport.model_validate_json(final_result)
			except ValidationError:
				match = re.search(r'```(?:json)?\s*(\{.*\})\s*```', final_result, re.DOTALL)
				if match:
					try:
						return BetalistReport.model_validate_json(match.group(1))
					except ValidationError:
						return None
		return None
	finally:
		try:
			await browser.kill()
		except Exception:
			pass


def parse_arguments() -> BetalistInput:
	parser = argparse.ArgumentParser(description='Extract the most recent startups listed on BetaList.')
	parser.add_argument(
		'--url',
		default='https://betalist.com/',
		help='BetaList landing page containing the grid (default: https://betalist.com/).',
	)
	parser.add_argument(
		'--last-days',
		type=int,
		default=3,
		help='Number of most recent days to include (default: 3).',
	)
	parser.add_argument(
		'--max-startups',
		type=int,
		default=200,
		help='Maximum number of startups to return (default: 200).',
	)
	parser.add_argument(
		'--output',
		default='betalist_recent.json',
		help='Output JSON file path (default: ./betalist_recent.json).',
	)
	args = parser.parse_args()
	return BetalistInput(
		url=AnyHttpUrl(args.url),
		last_days=args.last_days,
		max_startups=args.max_startups,
		output_path=Path(args.output),
	)


async def main() -> None:
	try:
		task_input = parse_arguments()
		report = await run_betalist_extraction(task_input)

		if report is None:
			print('‚ùå Aucun startup structur√© retourn√©.')
			return

		report_json = report.model_dump_json(indent=2, ensure_ascii=False)
		task_input.output_path.parent.mkdir(parents=True, exist_ok=True)
		task_input.output_path.write_text(report_json, encoding='utf-8')

		print(report_json)
		print(f'\n‚úÖ Export sauvegard√© dans: {task_input.output_path.resolve()}')
	except KeyboardInterrupt:
		print('\n‚ö†Ô∏è  Interruption utilisateur d√©tect√©e.')
	except Exception as exc:
		print(f"‚ùå Erreur lors de l'extraction: {exc}")
		raise


if __name__ == '__main__':
	asyncio.run(main())

